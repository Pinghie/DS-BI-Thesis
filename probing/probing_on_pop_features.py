# -*- coding: utf-8 -*-
"""Probing on pop features.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vL8o9zduUFoYbEa4xrPaa7nwETZq2VZq
"""

import spacy
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import pandas as pd

from sklearn.svm import LinearSVC
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from sklearn.preprocessing import MultiLabelBinarizer

from collections import Counter
from sklearn.metrics import make_scorer, mean_squared_error
from sklearn.model_selection import cross_val_score, cross_val_predict
from scipy import stats
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler

import warnings
warnings.filterwarnings("ignore")

def get_long_text_representation(text, tokenizer, model, max_length=512, stride=256):
  #Funziona senza troncare i testi dopo 512 token, calcolando la media di batch di testo
    inputs = tokenizer(text, return_tensors="pt", padding=False, truncation=False)
    input_ids = inputs["input_ids"].squeeze(0)
    num_tokens = input_ids.shape[0]

    embeddings = []
    for i in range(0, num_tokens, stride):
        chunk = input_ids[i : i + max_length].unsqueeze(0)
        with torch.no_grad():
            outputs = model(input_ids=chunk)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())

    return np.mean(embeddings, axis=0)

def get_representations(texts):
  #Funziona, ma troncando i testi dopo 512 token
    embeddings = []
    for text in texts:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
    return np.array(embeddings)

def probe_classification(X, Y):
  #Dati i due set X e Y, applica un modello di classificazione lineare
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
  svc = LinearSVC(dual=False, max_iter=10000)

  param_grid = {
      'C': [0.01, 0.1, 1, 10, 100],
      'loss': ['squared_hinge', 'hinge'],
      'penalty': ['l1', 'l2']
  }

  grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1, error_score=np.nan)
  grid_search.fit(X_train, y_train)

  classifier = grid_search.best_estimator_

  y_pred = classifier.predict(X_test)
  print(classification_report(y_test, y_pred))

df = pd.read_csv("annotated_texts_repr.csv", sep=",", encoding="utf-8")

dataset_X = list(df["text"])

dataset_Y_manichean = list(df["manichean"])
dataset_Y_peoplecentrism = list(df["peoplecentrism"])
dataset_Y_antielitism = list(df["antielitism"])
dataset_Y_emotional = list(df["emotional"])

dataset_Y_names = ["manichean", "peoplecentrism", "antielitism", "emotional"]
dataset_Y_list = [dataset_Y_manichean, dataset_Y_peoplecentrism, dataset_Y_antielitism, dataset_Y_emotional]

model_path = 'finetuned_bin_pop_model' #modello fine-tunato
model = AutoModel.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

dataset_Z = get_representations(dataset_X[:100])

for name, data in zip(dataset_Y_names, dataset_Y_list):
  print(name)
  probe_classification(dataset_Z, data[:100])

