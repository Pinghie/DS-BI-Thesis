# -*- coding: utf-8 -*-
"""Probing 2.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M1ZKfWB84vyjbfh9W0EWqgGGzvez1ieA
"""

# !python -m spacy download it_core_news_lg

import os
import json
import spacy
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import pandas as pd
from tqdm import tqdm

from sklearn.svm import LinearSVC
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from sklearn.preprocessing import MultiLabelBinarizer

from collections import Counter
from sklearn.metrics import make_scorer, mean_squared_error
from sklearn.model_selection import cross_val_score, cross_val_predict
from scipy import stats
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
import nltk

# nltk.download("stopwords")
nlp = spacy.load("it_core_news_lg")

def obtain_populist_keywords(df):
  #dato un df, aggrega i testi per populismo e restituisce una lista delle parole
  #con tfidf piÃ¹ alto per i testi populisti.

  merged_df = df.groupby('pop_sum', as_index=False).agg({'text': ' '.join}) #aggrego in due gruppi i testi populisti e non populisti
  texts = merged_df["text"]
  ita_stopwords = stopwords.words('italian')

  vectorizer = TfidfVectorizer(stop_words=ita_stopwords)
  tfidf_matrix = vectorizer.fit_transform(texts)

  feature_names = vectorizer.get_feature_names_out()

  top_n = 100
  top_common_words = []
  for i, text in enumerate(texts):
      tfidf_values = tfidf_matrix[i].toarray().flatten()
      top_indices = np.argsort(tfidf_values)[::-1][:top_n]
      top_words = [(feature_names[idx], tfidf_values[idx]) for idx in top_indices]
      top_common_words.append([feature_names[idx] for idx in top_indices])

  all_elements = top_common_words[0] + top_common_words[1]
  element_counts = Counter(all_elements)
  filtered_lists = [[x for x in lst if element_counts[x] == 1] for lst in [top_common_words[0], top_common_words[1]]]
  unpopulist_words = filtered_lists[0]
  populist_words = filtered_lists[1]

  return populist_words

def assign_labels(texts, keywords):
  #dati dei testi e delle keywords, assegna un punteggio ad ogni testo come frequenza relativa
  #delle keywords in quel testo
    scores = []
    for text in tqdm(texts):
        doc = nlp(text)
        word_count = len(doc)
        keyword_count = sum(1 for token in doc if token.text.lower() in keywords)
        score = (keyword_count / word_count if word_count > 0 else 0)*100
        scores.append(score)
    return scores

def get_long_text_representation(text, tokenizer, model, max_length=512, stride=256):
  #Funziona senza troncare i testi dopo 512 token, calcolando la media di batch di testo
    inputs = tokenizer(text, return_tensors="pt", padding=False, truncation=False)
    input_ids = inputs["input_ids"].squeeze(0)
    num_tokens = input_ids.shape[0]

    embeddings = []
    for i in range(0, num_tokens, stride):
        chunk = input_ids[i : i + max_length].unsqueeze(0)
        with torch.no_grad():
            outputs = model(input_ids=chunk)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())

    return np.mean(embeddings, axis=0)

def get_representations(texts):
  #Funziona, ma troncando i testi dopo 512 token
    embeddings = []
    for text in tqdm(texts):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to("cuda")
        with torch.no_grad():
            outputs = model(**inputs)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy())
    return np.array(embeddings)

def probe_regression(X, Y):
  #Dati i due set X e Y, applica un modello LinearRegression con k-fold
  scaler = StandardScaler()
  X = scaler.fit_transform(X)

  model = LinearRegression()
  cv_scores = cross_val_score(model, X, Y, cv=3, scoring='r2')  # R^2 Score
  cv_predictions = cross_val_predict(model, X, Y, cv=3)

  mse = mean_squared_error(dataset_Y, cv_predictions)
  spearman = stats.spearmanr(dataset_Y, cv_predictions)

  return {
    "crossvalidation_r2": cv_scores.tolist(),
    "mean_r2_score": cv_scores.mean(),
    "crossvalidated_mse": mse,
    "spearman": spearman.statistic.item(),
    "spearman_pvalue": spearman.pvalue.item()
  }

def probe_classification(X, Y):
  #Dati i due set X e Y, applica un modello di classificazione lineare
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
  svc = LinearSVC(dual=False, max_iter=10000)

  param_grid = {
      'C': [0.01, 0.1, 1, 10, 100],
      'loss': ['squared_hinge'],
      'penalty': ['l1', 'l2']
  }

  grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)
  grid_search.fit(X_train, y_train)

  classifier = grid_search.best_estimator_

  y_pred = classifier.predict(X_test)

  return classification_report(y_test, y_pred, output_dict=True)

df = pd.read_csv("annotated_texts_repr.csv", sep=",", encoding="utf-8")
df = df[["text", "pop_sum"]]
# df = df.iloc[:100]

df['pop_sum'] = df['pop_sum'].apply(lambda x: 0 if x < 2 else 1) #divido i testi in populisti e non populisti

keywords = obtain_populist_keywords(df)

dataset_X = list(df["text"])
if os.path.exists("labels.txt"):
  dataset_Y = pd.read_csv("labels.txt").values.astype(float).tolist()
else:
  dataset_Y = assign_labels(dataset_X, keywords)
  with open("labels.txt", "w") as log:
    log.write("label\n")
    log.writelines([str(s) + "\n" for s in dataset_Y])

model_path = 'finetuned_pol_model' #modello fine-tunato
model = AutoModel.from_pretrained(model_path).to("cuda")
tokenizer = AutoTokenizer.from_pretrained(model_path)

if os.path.exists("representations.npy"):
  with open("representations.npy", "rb") as log:
    dataset_Z =np.load(log)
else:
  dataset_Z = get_representations(dataset_X)
  with open("representations.npy", "wb") as log:
    np.save(log, dataset_Z)


#Versione senza truncation a 512 token:
#dataset_Z = np.array([get_long_text_representation(text, tokenizer, model) for text in dataset_X])

with open("regression.json", "w") as log:
  json.dump(probe_regression(dataset_Z, dataset_Y), log)

median_value = np.median(dataset_Y)
binary_representation = [0 if x < median_value else 1 for x in dataset_Y]

with open("classification.json", "w") as log:
  json.dump(probe_classification(dataset_Z, binary_representation), log)
